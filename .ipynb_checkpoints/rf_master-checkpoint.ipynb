{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Notebook: Bosques aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya vímos en scikit-learn gran parte de codigo es reciclable. Particularmente, leyendo variables y preparando los datos es lo mismo, independientemente del clasificador que usamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leyendo datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que no esta tan aburrido (también para mi) esta vez nos vamos a escribir una función para leer los datos que podemos reciclar para otros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def lea_datos(archivo, i_clase=-1, cabezazo=True, delim=\",\"):\n",
    "    '''Una funcion para leer archivos con datos de clasificación. \n",
    "    Argumentos:\n",
    "        archivo - direccion del archivo\n",
    "        i_clase - indice de columna que contiene las clases. \n",
    "                  default es -1 y significa la ultima fila.\n",
    "        header - si hay un encabezado\n",
    "        delim - que separa los datos\n",
    "    Regresa:\n",
    "        Un tuple de los datos, clases y cabezazo en caso que hay.'''\n",
    "    \n",
    "    todo = np.loadtxt(archivo, dtype=\"S\", delimiter=delim)    # para csv\n",
    "    if(cabezazo):\n",
    "        cabezazo = todo[0,:]\n",
    "        todo = todo[1:,:]\n",
    "    else: \n",
    "        cabezazo = None\n",
    "    \n",
    "    clases = todo[:, i_clase]\n",
    "    datos = np.delete(todo, i_clase, axis=1)\n",
    "    print \"Clases\"\n",
    "    for k,v in Counter(clases).iteritems(): print k,\":\",v\n",
    "    return (datos, clases, cabezazo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora importando datos se hace muy simple en el futuro. Para datos de csv con cabezazo por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos, clases, cabeza = lea_datos(\"titanic.csv\")  # _ significa que no nos interesa este valor \n",
    "clases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceando datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizacion de datos no es necesario para bosques aleatorios o arboles porque son invariantes al respeto de los magnitudes de variables. Lo unico que podría ser un problema es los clases son imbalanceados bajo nuestra percepción. Esto significa que a veces ni esperamos que nuestros datos van a ser balanceados y, en este caso, es permisible dejarlos sin balancear porque queremos que el clasificador incluye este probabilidad \"prior\" de la vida real. Si por ejemplo clasifico por ejemplo evaluaciones del vino, puede ser que lo encuentro normal que muchos vinos tienen una evaluacion promedia y solamente poco una muy buena. Si quiero que mi clasificador también tenga esta \"tendencía\" no tengo que balancear. \n",
    "En otro caso, si diseño una prueba para una enfermeda y no quiero que mi clasificador hace asumpciones sobre el estado del paciente no basados en mis variables, mejor balanceo.\n",
    "\n",
    "Scikit-learn no llega con una función para balancear datos entonces porque no lo hacemos nosotros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def balance(datos, clases, estrategia=\"down\"):\n",
    "    '''Balancea unos datos así que cada clase aparece en las mismas proporciones.\n",
    "    Argumentos:\n",
    "        datos - los datos. Filas son muestras y columnas variables.\n",
    "        clases - las clases para cada muestra.\n",
    "        estrategia - \"up\" para up-scaling y \"down\" para down-scaling'''\n",
    "    \n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Decidimos los nuevos numeros de muestras para cada clase\n",
    "    conteos = Counter(clases)\n",
    "    if estrategia==\"up\": muestras = max( conteos.values() )\n",
    "    else: muestras = min( conteos.values() )\n",
    "    \n",
    "    datos_nuevo = np.array([]).reshape( 0, datos.shape[1] )\n",
    "    clases_nuevo = []\n",
    "    \n",
    "    for c in conteos:\n",
    "        c_i = np.where(clases==c)[0]\n",
    "        new_i = np.random.choice(c_i, muestras, replace=(estrategia==\"up\") )\n",
    "        datos_nuevo = np.append( datos_nuevo, datos[new_i,:], axis=0 )\n",
    "        clases_nuevo = np.append( clases_nuevo, clases[new_i] )\n",
    "    \n",
    "    return (datos_nuevo, clases_nuevo)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y vamos a ver si funciona..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos_b, clases_b = balance(datos, clases)\n",
    "print Counter(clases_b)\n",
    "print datos_b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando el bosque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corriendo un bosque aleatorio para la clasificación es casi lo mismo como todos los otros clasificadores. Nada más importamos de una libreria diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(oob_score=True)\n",
    "clf.fit(datos_b, clases_b)\n",
    "print \"Exactitud estimado:\", clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya sabemos que los hiper-parametros mas importantes para los bosques son el numero de arboles dado por `n_estimators` y la profundidad del arbol dado por `max_depth`. Porque son monotonos los podemos variar por separado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv = StratifiedKFold(y=clases, n_folds=4)\n",
    "arboles_vals = np.arange(5,200,5)\n",
    "busqueda = GridSearchCV(clf, param_grid=dict(n_estimators=arboles_vals), cv=cv)\n",
    "busqueda.fit(datos, clases)\n",
    "\n",
    "print 'Mejor numero de arboles=',busqueda.best_params_,',exactitud =',busqueda.best_score_\n",
    "\n",
    "scores = [x[1] for x in busqueda.grid_scores_]\n",
    "\n",
    "plt.plot(arboles_vals, scores)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('exactitud cv')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para la profundidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prof_vals = np.arange(1,12)\n",
    "busqueda = GridSearchCV(clf, param_grid=dict(max_depth=prof_vals), cv=cv)\n",
    "busqueda.fit(datos, clases)\n",
    "\n",
    "print 'Mejor profundidad=',busqueda.best_params_,',exactitud =',busqueda.best_score_\n",
    "\n",
    "scores = [x[1] for x in busqueda.grid_scores_]\n",
    "plt.plot(prof_vals, scores)\n",
    "plt.xlabel('profundidad maxima')\n",
    "plt.ylabel('exactitud cv')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo unas buenas valores podemos escoger un bosque con la menor numero de arboles y profundidad para una buena exactitud. También esta vez vamos a sacar las importancias de variables de una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=101, oob_score=True) \n",
    "clf.fit(datos, clases)\n",
    "print \"Exactitud estimada:\", clf.oob_score_\n",
    "\n",
    "print cabeza\n",
    "print clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predecir nuevas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos_test, clases_test, _ = lea_datos(\"titanic_test.csv\")\n",
    "clases_pred = clf.predict(datos_test)\n",
    "print \"Predicho:\", clases_pred\n",
    "print \"Verdad:  \", clases_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
