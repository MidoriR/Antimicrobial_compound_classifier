{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook sirve como plantilla para solver un problema de clasificación. Partes que tienes que llenar tu son maracdos con **?**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leyendo datos y pre-processamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código siguiente va a asumir que los datos son una mezcla de palabras/letras y numeros. En caso que los datos son numeros desde el principio se puede omitir el `dtype='S'` y se van a leer los datos como `float`. No olvides ajustar el `delimiter` dependiendo de tus datos (omitirlo usa espacios o tabs).\n",
    "\n",
    "Para ver como se puede escoger partes de un matrix en numpy también ayuda ver [la introducción de numpy](http://wiki.scipy.org/Tentative_NumPy_Tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "todo = np.loadtxt('???', dtype='S', delimiter=',')    # asumimos formato csv\n",
    "header = todo[0,:]                    # solamente cuando las columnas tienen titulo\n",
    "datos = todo[?,?].astype('float')     # depende de tus datos por ejemplo sin titulos y sin la ultima fila es todo[1:,0:-1]\n",
    "clases = todo[?,-1]  \n",
    "\n",
    "print 'dimensiones datos (filas, columnas) =', datos.shape\n",
    "print '#muestras =', len(clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto te debería dejar con los datos (mediciones de variables) y clases. Ahora los vamos a estandardizar. Para métodos de normalizar datos también [vea aquí](http://scikit-learn.org/stable/modules/preprocessing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datos)\n",
    "datos_norm = scaler.transform(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí vamos a clasificar los datos con SVC. Para más información vea la documentación para los [SVMs](http://scikit-learn.org/stable/modules/svm.html) y la [validación cruzada](http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "\n",
    "clf = SVC(kernel='rbf', class_weight=???)\n",
    "exas = cross_val_score(clf, datos_norm, clases, cv=8)    # escoge un numero adecuado de folds \n",
    "print 'cv exactitud =', exas.mean(), '±', exas.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...y para ver los hiper-parámetros (ejemplarios para dos hiper-parámetros) procedemos con el siguiente código. Ten cuidado aquí porque podría tomar bastante tiempo aquí. Si tienes muchas muestras puede ser buena estrategía hacer todo nada mas para unos de tus datos. Si tienes `n` muestras puedes sacar una selecion aleatoria de 100 elementos así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = ???\n",
    "sel = np.random.permutation(n)[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con esto puedes correr tu analisis de hiper-parametros..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv = StratifiedKFold(y=clases[sel], n_folds=3)     # usamos las primeras 100 muestras\n",
    "C_rango = 10.0 ** np.arange(-3,4)\n",
    "gamma_rango = 10.0 ** np.arange(-3,4)\n",
    "busqueda = GridSearchCV(clf, param_grid=dict(C=C_rango, gamma=gamma_rango), cv=cv)\n",
    "busqueda.fit(datos_norm[sel,], clases[sel])        # usamos las primeras 100 muestras\n",
    "\n",
    "print 'Mejor C, gamma =',busqueda.best_params_,',exactitud =',busqueda.best_score_\n",
    "\n",
    "scores = np.array( [x[1] for x in busqueda.grid_scores_] ) # Sacamos nada mas el segundo elemento de los scores\n",
    "n_C, n_g = len(C_rango), len(gamma_rango)                  # sacamos el numero de parametros probados\n",
    "scores = scores.reshape(n_C, n_g)                          # convertimos los scores en la cuadricula\n",
    "\n",
    "# Ahora vamos a dibujar todo\n",
    "plt.figure(figsize=(10,8))                     # hacemos el imagen un poco mas grande\n",
    "plt.imshow(scores, interpolation='nearest')    # dibuja los scores representado por colores\n",
    "plt.xlabel('gamma')                            # pon el titulo del eje x\n",
    "plt.ylabel('C')                                # pon el titulo del eje y\n",
    "plt.colorbar()                                 # pon un escala de colores para los scores\n",
    "plt.xticks(np.arange(len(gamma_rango)), gamma_rango, rotation=45)    # pon los valores para gamma\n",
    "plt.yticks(np.arange(len(C_rango)), C_rango)   # pon los valores para C\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso que nada más es un hiper-parámetro los paso del reshape no son necesarios y deberías usar `plt.plot` en lugar de `plt.imshow`.\n",
    "\n",
    "Ahora podemos sacar el mejor clasificador..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = busqueda.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediciendo datos no es muy complicado. Entrenamos el SVM con todos los datos y metemos nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos_nuevo = ???     # tú tienes que sacarlos...\n",
    "\n",
    "datos_nuevo_norm = scaler.transform(datos_nuevos)\n",
    "clf.fit(datos_norm, clases)         # entrenamos con todo que tenemos\n",
    "clases_nuevo = clf.predict(datos_nuevo_norm)\n",
    "\n",
    "print 'Clases predichos: ', clases_nuevo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
